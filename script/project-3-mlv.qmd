---
title: |
  Computer Intensive Methods
  Final project 3
author:   
  - Luong Vuong (2365900)
  - Ilse Hamers (2366276)
  - Joseph Kaunda (2364031)
format: 
  pdf:
    documentclass: article
    fig-pos: 'b'
editor: visual
geometry:
      - inner=1cm
      - outer=1cm
      - top=1cm
      - bottom=1cm
---

```{r load the necessary libraries}
#| echo: false
#| warning: false

library(tidyverse)
library(Ecdat)
library(caret)
library(boot)
library(patchwork)

```

```{r data manipulation}
#| include: false
#| warning: false
#| output: false

# data contains information over an experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens
head(chickwts)
str(chickwts)

```

**Research question**: Is there a difference between the chicks' weights across the diet groups?

# Question 1

## Formulate a one-way ANOVA model for the problem

The one-way ANOVA model is:

$$
Y_{ij} = \mu + \alpha_j + \epsilon_{ij} \quad \text{for} \quad i = 1, \ldots, 71; \quad j = 1, \ldots, 6; \quad \epsilon_{ij} \sim \text{iid} \, N(0, \sigma^2)
$$

where $Y_{ij}$ is the $i$-th weight from the $j$-th feed, and $\alpha_j$ is the effect for the $j$-th feed group.

## Formulate the null hypothesis and the alternative

The hypotheses associated with the model are:

$$
H_0 : \alpha_1 = \alpha_2 = \cdots = \alpha_6 = 0
$$ $$
H_1 : \text{at least one } \alpha_j \text{ is different from } 0
$$

## Formulate the test statistic

The test statistic is given as:

$$
F = \frac{\sum_{j=1}^6 n_j (\bar{x}_j - \bar{x})^2 / 5}{\sum_{i=1}^{n_j} \sum_{j=1}^6 (x_{ij} - \bar{x}_j)^2 / 65}
$$

and the critical value is found in a table of probability values for the F distribution with degrees of freedom $\text{df}_1 = 5$ and $\text{df}_2 = 65$. Also, $n_j$ is the sample size in the $j$-th group, $\bar{x}_j$ is the sample mean of the $j$-th group, and $\bar{x}$ is the overall mean.

```{r fig-lm-res}
#| echo: false
#| warning: false
#| label: fig-box-chick
#| fig-cap: "Boxplot of the feeds on weight of chicks"
#| fig-pos: H
#| fig-width: 5
#| fig-height: 3

ggplot(data = chickwts, aes(x = feed, y = weight)) + 
  geom_boxplot() + theme_classic()
```

The box plot in @fig-box-chick shows the feeds on the weights of the chicks. It seems that the median weight of casein feed is higher than others and the weight of horsebean feeds is the lowest. 

## Test the null hypothesis using the classical $F$-test and test the null hypothesis using a significance level of 5%.

| Source    | df  | Sum Sq    | Mean Sq  | F value | Pr(\>F) |
|-----------|-----|-----------|----------|---------|---------|
| feed      | 5   | 231129.16 | 46225.83 | 15.36   | 0.00001 |
| Residuals | 65  | 195556.02 | 3008.55  |         |         |

: Analysis of variance result {#tbl-anova}

The analysis of variance result is shown in table @tbl-anova. The F statistic (15.36) has p value (0.00001) which is less than 0.05. This implies that we reject the null hypothesis and conclude that the effect of the feeds are not the same on the chicks weight.

## Test the null hypothesis of no diet effect with semi- parametric bootstrap

```{r anova-chick}
#| include: false
#| warning: false
#| output: false

chicken <- chickwts
fit <- aov(weight ~ feed, data = chicken)
summary(fit)

```

In this question we fitted a linear regression model of weight against feed, with feed a categorical variable with 6 levels, $y_i = \beta_0 + \beta_1 I(x_i = 2) + \beta_2 I(x_i = 3) + \beta_3 I(x_i = 4) + \beta_4 I(x_i = 5) + \beta_5 I(x_i = 6) + \varepsilon_i$. To implement the semi-parametric bootstrap procedure for inference, in the first step we fit the null model and calculate the residuals under the null, $e_{0,i}$. Note that the null hypothesis $H_0: \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0$ implies that $y_i = \beta_0 + \epsilon_i$.

The semi-parametric bootstrap loop for inference ($H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$) consists of the following steps:

1.  Resample the residuals vector ($e_{0,i}$), that were obtained under $H_0$.

2.  Calculate the bootstrap replicates for the response $y^*_1, \dots, y^*_n$ under the null hypothesis in the following way:

    $$
    y^*_i = \hat{\beta}_0 + e^*_{0,i}
    $$ Here we performed 1000 bootstraps. We then calculated the Monte Carlo p value based on this formula $\frac{1 + \# \left( \left| \beta_{i,b} \right| \geq \left| \beta_{i,\text{obs}} \right| \right)}{B + 1}, \, i = 1, 2, 3, 4, 5$

```{r semi-boot-chick}
#| include: false
#| warning: false
#| output: false

fit.lm <- lm(weight~feed, data = chicken)

beta0 <- summary(fit.lm)$coeff[1,1]
beta1 <- summary(fit.lm)$coeff[2,1]
beta2 <- summary(fit.lm)$coeff[3,1]
beta3 <- summary(fit.lm)$coeff[4,1]
beta4 <- summary(fit.lm)$coeff[5,1]
beta5 <- summary(fit.lm)$coeff[6,1]

y <- chicken$weight
x <- chicken$feed

fit.lm.0 <- lm(y ~ 1)
summary(fit.lm.0)
ei.0 <- fit.lm.0$resid

n <- length(x)
B <- 1000
beta0.b <- beta1.b <- beta2.b <- beta3.b <- beta4.b <- beta5.b <- c(1:B)
for (i in 1:B) {
    e.boot <- sample(ei.0, size = n, replace = T)
    y.boot <- fit.lm.0$coeff[1] + e.boot
    x.boot <- x
    fit.boot <- lm(y.boot ~ x.boot)
    beta0.b[i] <- fit.boot$coeff[1]
    beta1.b[i] <- fit.boot$coeff[2]
    beta2.b[i] <- fit.boot$coeff[3]
    beta3.b[i] <- fit.boot$coeff[4]
    beta4.b[i] <- fit.boot$coeff[5]
    beta5.b[i] <- fit.boot$coeff[6]
}

p_1 <- (1 + sum(abs(beta1.b) >= abs(beta1))) / (B + 1)
p_2 <- (1 + sum(abs(beta2.b) >= abs(beta2))) / (B + 1)
p_3 <- (1 + sum(abs(beta3.b) >= abs(beta3))) / (B + 1)
p_4 <- (1 + sum(abs(beta4.b) >= abs(beta4))) / (B + 1)
p_5 <- (1 + sum(abs(beta5.b) >= abs(beta5))) / (B + 1)

```

The resulted p-values for $\beta_1, \beta_2, \beta_3, \beta_4, \beta_5$ are `r round(p_1, 3)`, `r round(p_2, 3)`, `r round(p_3, 3)`, `r round(p_4, 3)`, `r round(p_5, 3)`, respectively. As 3 out of 5 p values are \< 0.05, we reject the null hypothesis.

## Permutations test to test the null hypothesis of no diet effect

Here performed the linear regression model again where we did 1000 bootstraps, and sampling without replacement with the outcome variable weight. Finally we calculated the monte carlo p value based on the observed F statistic and the F statistics from all the bootstraps. The formula is similar as above.

```{r permute-test}
#| include: false
#| warning: false
#| output: false

# number of bootstrap
N <- 1000
PermuteFunction <- function(y = chicken$weight, x = chicken$feed) {
  model.resample = lm(sample(y, replace = F) ~ x)
  fstats = summary(model.resample)$fstat[1]
  return(fstats)
}

fstats = numeric(N)

for (i in 1:N) {
  fstats[i] <- PermuteFunction()
}

p_permute <- (1 + length(fstats[fstats >= summary(fit.lm)$fstat[1]]))/(N + 1)
p_permute

```

The resulted p value is `r round(p_permute, 3)`, indicating a significant result. This proves that we reject the null hypothesis of no diet effect.

## Estimate mean difference between the Sunflower and Soybean diet groups and its 90% CI using a parametric bootstrap

```{r soybean sunflower}
#| include: false
#| warning: false
#| output: false

cc <- chicken  |> 
  filter(feed %in% c("soybean", "sunflower"))

cc <- cc |> 
  mutate(feed_dummy = ifelse(feed == "sunflower", 1, 0))

set.seed(123)

# fit model
fit.lm <- lm(weight ~ feed_dummy, data = cc)
beta0 <- summary(fit.lm)$coeff[1,1]
beta1 <- summary(fit.lm)$coeff[2,1]
sigma <- (summary(fit.lm))$sigma

# bootstrap
B <- 1000
beta0.b <- beta1.b <- c(1:B)
n <- length(cc$feed)
weight.b <- c(1:n)
for (i in 1:B) {
  for (j in 1:n) {
    weight.b[j] <- rnorm(1, beta0 + beta1*cc$feed_dummy[j], sigma)
  }
  fit.lm.b <- lm(weight.b ~ cc$feed_dummy)
  beta0.b[i] <- summary(fit.lm.b)$coeff[1,1]
  beta1.b[i] <- summary(fit.lm.b)$coeff[2,1]
}
quantile(beta1.b, probs = c(0.05, 0.95))

```
For parametric bootstrap, we first fit the linear model between weight and feed_dummy, where feed_dummy equals 1 if the diet group is sunflower, and 0 if it is soybean. Thus, estimating the mean difference between two diet groups becomes estimating the slope of the regression model. We estimated the unknown parameters ${\beta}_{0}$, ${\beta}_{1}$ and $\sigma^{2}$ from that model, and within the bootstrap loop we resampled from $N(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i},\hat{\sigma}^2)$  using the R function rnorm(n,mu,sigma).

With that, we derived the bootstrap parametric distribution of $\hat{\beta}_{1}$ with 1000 bootstraps. **Conclusion:** The mean and the corresponding 90% confidence interval of the mean weight difference between the two feeds group sunflower and soybean are `r round(mean(beta1.b), 3)`, `r round(quantile(beta1.b, probs = c(0.05, 0.95))[1], 3)` and `r round(quantile(beta1.b, probs = c(0.05, 0.95))[2], 3)`, respectively.

# Question 2

```{r computers dataset}
#| include: false
#| warning: false
#| output: false

data("Computers") 
names(Computers)
head(Computers)

```

## Estimate the model using the classical OLS approach.

We simply fitted a linear regression model between price and size of hard drive (hd)

```{r lm computers}
#| include: false
#| warning: false
#| output: false

computer <- lm(price ~ hd, data = Computers)
summary(computer)
```

As the hard drive size increases by 1 MB, the price of the 486 PCs will increase by `r round(computer[["coefficients"]][["hd"]], 3)`, the effect is significant (p value = `r round(summary(computer)$coefficients["hd", "Pr(>|t|)"], 3)`).

## Predict the price and estimate the prediction error

We used the predict() function to predict the price from the model.

```{r predict price}
#| include: false
#| warning: false
#| output: false

price_predict <- predict(computer, newdata = Computers)
```

The predicted price has a mean of `r round (mean(price_predict), 0)` and ranges from `r round (min(price_predict), 0)` to `r round (max(price_predict), 0)`.

In this linear regression model, the prediction error is quantified by the residual standard error (RSE), a.k.a the root mean squared error (RMSE), which is the square root of the mean of the squared residuals. And the RSE equals `r round(summary(computer)$sigma, 3)` in our model.

```{r mean rss computer}
#| include: false
#| warning: false
#| output: false

summary(computer)$sigma

```

## 10-fold cross-validation

To perform the 10 fold cross-validation, we first split the sample into 10 equal parts. Then for each kth part (k = 1, ..., 10), fit the model to the other 9 parts and calculate the prediction error of the fitted model with the kth part.

```{r 10-fold cross validation}
#| include: false
#| warning: false
#| output: false

# Define the 10-fold cross-validation method
train_control <- trainControl(method = "cv", number = 10)

# Fit the linear model using 10-fold cross-validation
model_cv <- train(price ~ hd, data = Computers, method = "lm", trControl = train_control)

# Calculate the prediction error (RMSE or other metrics)
prediction_error <- model_cv$results$RMSE

```
The RMSE obtained from the 10 fold cross validation is `r round(prediction_error, 3)`, which is similar to the prediction error obtained from the model.

## Leave one out cross validation

We implement a leave-one-out-cross-validation procedure for the Computers dataset. At each step, one observation is left out and the regression model is fitted to the n−1 observations, and after which the slope is calculated. As there are 6259 observations, 6259 cross-validations were performed.

```{r loocv}
#| include: false
#| warning: false
#| output: false

n <- length(Computers$price)
beta.cv <- fit.cv <- c(1:n)
x <- Computers$hd
y <- Computers$price
for (i in 1:n) {
    x.cv <- x[ -c(i)]
    y.cv <- y[ -c(i)]
    fit.lm.cv <- lm(y.cv ~ x.cv)
    beta.cv[i] <- fit.lm.cv$coeff[2]
}

```

```{r fig-loocv}
#| echo: false
#| warning: false
#| label: fig-loocv
#| fig-pos: H
#| fig-width: 5
#| fig-height: 3

plot(beta.cv)

```
Figure @fig-loocv shows that the slope remains stable across all the cross-validations, except for the cross-validation with a large index (5000+) where there are some larger slope values. Also, few observations at a lower index (around 1000 and 2000) have smaller slopes. These values suggest that there might be influencing observations in the dataset. It is important to note that the differcene of these "outlying observations" with the overall trend is rather small (about 0.004).

## Bootstrap for the 95% of the predicted values

Here we use a non-parametric bootstrap procedure to calculate the 95% CI of the predicted values of the model with 1000 bootstraps. And in this case, we need to sample pair.

```{r 95% predicted value}
#| include: false
#| warning: false
#| output: false
set.seed(123)
n <- length(Computers$price)
index <- c(1:n)
B <- 1000
beta0.b <- beta1.b <- predict.b <- c(1:B)
computers <- Computers |> 
  select(price, hd)
for (i in 1:B) {
index.b <- sample(index, n, replace = TRUE)
computers.b <- computers[index.b, ]
fit.lm.b <- lm(computers.b$price~computers.b$hd)
predict.b[i] <- predict(fit.lm.b, newdata = computers)
beta0.b[i] <- summary(fit.lm.b)$coeff[1,1]
beta1.b[i] <- summary(fit.lm.b)$coeff[2,1]
}
quantile(beta0.b, probs = c(0.025,0.975))
quantile(beta1.b, probs = c(0.025,0.975))
```

The 95% equal tail CI resulting from the non-parametric bootstrap for the intercept $\beta_0$ is given by `r round(quantile(beta0.b, probs = c(0.025, 0.975)), 3)`, and for the slope $\beta_1$, the 95% equal tail CI is `r round(quantile(beta1.b, probs = c(0.025, 0.975)), 3)`.

# Question 3

## 95% C.I for SE(beta0) and SE(beta1)

Here we use a non-parametric bootstrap procedure to calculate the 95% C.I for $\text{SE}(\hat{\beta}_0)$ and $\text{SE}(\hat{\beta}_1)$ with 1000 bootstraps. And in this case, we also need to sample pair.

```{r 95% CI coeffs}
#| include: false
#| warning: false
#| output: false
set.seed(123)
n <- length(Computers$price)
index <- c(1:n)
B <- 1000
sebeta0.b <- sebeta1.b <- c(1:B)
computers <- Computers |> 
  select(price, hd)
for (i in 1:B) {
index.b <- sample(index, n, replace = TRUE)
computers.b <- computers[index.b, ]
fit.lm.b <- lm(computers.b$price~computers.b$hd)
sebeta0.b[i] <- summary(fit.lm.b)$coeff[1,2]
sebeta1.b[i] <- summary(fit.lm.b)$coeff[2,2]
}
quantile(sebeta0.b, probs = c(0.025,0.975))
quantile(sebeta1.b, probs = c(0.025,0.975))
```

The 95% equal tail CI resulting from the non-parametric bootstrap for the $\text{SE}(\hat{\beta}_0)$ is given by `r round(quantile(sebeta0.b, probs = c(0.025, 0.975)), 3)`, and for the slope $\text{SE}(\hat{\beta}_1)$, the 95% equal tail CI is `r round(quantile(sebeta1.b, probs = c(0.025, 0.975)), 3)`.

## Influential observations

To identify the influence of observations for which the hard drive size is larger than 2000 MB, we used 1000 bootstrap to fit the model to the data without these observations. We investigated how removing such observations would have an effect on both $\text{SE}(\hat{\beta}_0)$ and $\text{SE}(\hat{\beta}_1)$ by comparing these estimates to the above estimates obtained from the complete data.  

```{r influ obs}
#| include: false
#| warning: false
#| output: false
set.seed(123)
computers_2k <- Computers |> 
  filter(hd <= 2000)
n <- length(computers_2k$price)
index <- c(1:n)
B <- 1000
sebeta0.b <- sebeta1.b <- c(1:B)
computers <- computers_2k |> 
  select(price, hd)
for (i in 1:B) {
index.b <- sample(index, n, replace = TRUE)
computers.b <- computers[index.b, ]
fit.lm.b <- lm(computers.b$price~computers.b$hd)
sebeta0.b[i] <- summary(fit.lm.b)$coeff[1,2]
sebeta1.b[i] <- summary(fit.lm.b)$coeff[2,2]
}
quantile(sebeta0.b, probs = c(0.025,0.975))
quantile(sebeta1.b, probs = c(0.025,0.975))

```
The resulted 95% equal tail CI resulting from the non-parametric bootstrap for the $\text{SE}(\hat{\beta}_0)$ and $\text{SE}(\hat{\beta}_1)$ after removing observations with hard drive size > 2000 MB are `r round(quantile(sebeta0.b, probs = c(0.025, 0.975)), 3)` and `r round(quantile(sebeta1.b, probs = c(0.025, 0.975)), 3)`. These are similar to the ones obtained before, proving that the observations with large hard drive size are not influential.

# Question 4

```{r loading data}
#| include: false
#| warning: false
#| output: false
x <- c(0.68446806,-0.02596037,-0.90015774,0.72892605,-0.45612255, 0.19311847, -0.13297109, -0.99845382, 0.37278006, -0.20371894, -0.15468803, 0.19298230, -0.42755534, -0.04704525, 0.15273726, 0.03655799, 0.01315016, -0.59121428, 4.50955771, 2.87272653) 
length(x)

```
## Estimate 𝜇 using the mean and the median.

The estimated parameter $\hat{\mu}$ using the mean and the median are `r round(mean(x), 3)` and `r round(median(x), 3)`, respectively.

## Approximate the distribution of the sample mean and the median using non parametric bootstrap with B=1000

```{r 1k boot mean med}
#| include: false
#| warning: false
#| output: false
set.seed(123)
n <- length(x)
B <- 1000
mean.b <- median.b <- c(1:B)
for (i in 1:B) {
x.b <- sample(x, n, replace = TRUE)
mean.b[i] <- mean(x.b)
median.b[i] <- median(x.b)
}

```

```{r fig-hist-mean-med}
#| echo: false
#| warning: false
#| label: fig-hist-mean-med
#| fig-cap: "The approximate distribution of the sample mean and median"
#| fig-pos: H
#| fig-width: 5
#| fig-height: 3
hist_mean <- ggplot(data.frame(mean.b), aes(x = mean.b)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(x = "Sample mean", y = "Frequency") +
  theme_classic()

hist_med <- ggplot(data.frame(median.b), aes(x = median.b)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(x = "Sample median", y = "Frequency") +
  theme_classic()
hist_mean + hist_med

```
The approximate distribution of the sample mean and median using 1000 non-parametric bootstrap is shown in figure @fig-hist-mean-med. It appears that the sample median estimate has a smaller standard error.

## Estimate the standard error of the sample mean and the median and calculate 95% C.I for the sample mean and median using a semi parametric bootstrap

```{r semi-boot-mean-med}
#| include: false
#| warning: false
#| output: false
# Number of bootstrap samples
B <- 1000

# Fit a linear model (mean model)
fit <- lm(x ~ 1)

# Extract residuals
residuals <- fit$residuals

# Initialize vectors to store bootstrap estimates
bootstrap_means <- numeric(B)
bootstrap_medians <- numeric(B)

# Perform semi-parametric bootstrap
set.seed(123)  # For reproducibility
for (i in 1:B) {
  # Resample residuals
  resampled_residuals <- sample(residuals, length(x), replace = TRUE)
  
  # Create bootstrap sample
  bootstrap_sample <- fit$fitted.values + resampled_residuals
  
  # Calculate mean and median of the bootstrap sample
  bootstrap_means[i] <- mean(bootstrap_sample)
  bootstrap_medians[i] <- median(bootstrap_sample)
}

# Estimate standard errors
sd(bootstrap_means)
sd(bootstrap_medians)

# Calculate 95% confidence intervals
quantile(bootstrap_means, c(0.025, 0.975))
quantile(bootstrap_medians, c(0.025, 0.975))
```
For this question we did a semi parametric bootstrap where we first fitted an intercept-only linear regression model for x. We then sampled the residuals from that model with replacement with a total of 1000 bootstraps. In each bootstrap, a new sample of x was calculated by adding the sampled residuals to the fitted value from the model. Finally, sample mean, median, and their standard errors were calculated from the bootstrap samples. The estimated standard error of the sample mean and the median are `r sd(bootstrap_means)` and `r sd(bootstrap_medians)`, respectively. Additionally, the equal tail 95% C.I. for these parameters are `r round(quantile(bootstrap_means, c(0.025, 0.975)), 3)` and `r round(quantile(bootstrap_medians, c(0.025, 0.975)), 3)`, respectively.

## Estimate the MSE for the mean and the median using jackknife, which parameter estimate you prefer to use?

The jackknife is a resampling procedure that can be used to calculate the standard error or bias of an estimate. In each resampling step of the jackknife loop, one observation from the original sample is left out and the plug in estimate is calculated. It is important to take into account the inflation factor into the jackknife estimate of the standard error since the jackknife replicates have less variability around the parameter mean estimate due to the fact that at each iteration only one observation is taken out from the observed sample. Therefore, if we do not use the inflation factor we will under estimate the variability of the parameter estimate. The formula is $SE\hat{\theta}=\frac{n-1}{n}\sum_{i=1}^{n} \left ( \hat{\theta}^{(-i)}- \hat{\theta}^{(.)} \right )^{2}$

A Jackknife procedure for estimating the MSE for the mean and the median of x is implmented using a “for loop”" in which at each set of the loop one observation is left out of the data. To calculate MSE, the bias and variance of the jacknife estimates were calculated. The estimates for the bias and for the variance are $(n-1) \left ( \hat{\theta}^{(.)}- \hat{\theta} \right )$ and $SE\hat{\theta}=\frac{n-1}{n}\sum_{i=1}^{n} \left ( \hat{\theta}^{(-i)}- \hat{\theta}^{(.)} \right )^{2}$, respectively. MSE was then calculated by the bias squared + variance.

```{r jackknife mean-med}
#| include: false
#| warning: false
#| output: false
n <- length(x)
mean.jack <- median.jack <- c(1:n)
 for (i in 1:n) {
    x.jack <- x[ -c(i)]
    mean.jack[i] <- mean(x.jack)
    median.jack[i] <- median(x.jack)
 }

# Jackknife estimate of the mean and median
jackknife_mean <- mean(mean.jack)
jackknife_median <- median(median.jack)

# Jackknife bias
bias_jackknife_mean <- (n - 1) * (jackknife_mean - mean(x))
bias_jackknife_median <- (n - 1) * (jackknife_median - median(x))

# Jackknife variance
var_jackknife_mean <- (n - 1) * mean((mean.jack - jackknife_mean)^2)
var_jackknife_median <- (n - 1) * mean((median.jack - jackknife_median)^2)

# Mean Squared Error (MSE)
mse_jackknife_mean <- bias_jackknife_mean^2 + var_jackknife_mean
mse_jackknife_median <- bias_jackknife_median^2 + var_jackknife_median

```
The MSE estimate for the median is `r round(mse_jackknife_median, 3)`, which is way smaller than the MSE estimate for the mean (`r round(mse_jackknife_mean, 3)`). This makes the median a preferred estimator.

## Let $M$ be the median and let $\pi(M < 0) = P(M < 0).$ Estimate $\pi(M < 0)$, estimate the distribution of $\hat{\pi}(M < 0)$, and construct a 95% C.I. for $\pi(M < 0)$.

To answer this question, we needed to do a nested foor loop where we first did a 1000 bootstrap to get the distribution of the sample median, and from each distribution we can estimate the probability that the median is <0. We then loop over this 1000 times to get the distribution of the probablity.

```{r prob M<0}
#| include: false
#| warning: false
#| output: false
set.seed(123)
N <- 1000
prob.b <- c(1:N)
for (j in 1:N) {
n <- length(x)
B <- 1000
median.b <- c(1:B)
for (i in 1:B) {
x.b <- sample(x, n, replace = TRUE)
median.b[i] <- median(x.b)
}
prob.b[j] <- mean(median.b < 0)
}

```
The 95% equal tail CI of the $\hat{\pi}_{(M < 0)}$ are `r round(quantile(prob.b, probs = c(0.025, 0.975)), 3)`.

# Syntax

```{r code syntax}
#| output: false
#| warning: false
#| eval: false
library(tidyverse)
library(Ecdat)
library(caret)
library(boot)
library(patchwork)

# Question 1

head(chickwts)
str(chickwts)

ggplot(data = chickwts, aes(x = feed, y = weight)) + 
  geom_boxplot() + theme_classic()

chicken <- chickwts
fit <- aov(weight ~ feed, data = chicken)
summary(fit)

fit.lm <- lm(weight~feed, data = chicken)

beta0 <- summary(fit.lm)$coeff[1,1]
beta1 <- summary(fit.lm)$coeff[2,1]
beta2 <- summary(fit.lm)$coeff[3,1]
beta3 <- summary(fit.lm)$coeff[4,1]
beta4 <- summary(fit.lm)$coeff[5,1]
beta5 <- summary(fit.lm)$coeff[6,1]

y <- chicken$weight
x <- chicken$feed

fit.lm.0 <- lm(y ~ 1)
summary(fit.lm.0)
ei.0 <- fit.lm.0$resid

n <- length(x)
B <- 1000
beta0.b <- beta1.b <- beta2.b <- beta3.b <- beta4.b <- beta5.b <- c(1:B)
for (i in 1:B) {
    e.boot <- sample(ei.0, size = n, replace = T)
    y.boot <- fit.lm.0$coeff[1] + e.boot
    x.boot <- x
    fit.boot <- lm(y.boot ~ x.boot)
    beta0.b[i] <- fit.boot$coeff[1]
    beta1.b[i] <- fit.boot$coeff[2]
    beta2.b[i] <- fit.boot$coeff[3]
    beta3.b[i] <- fit.boot$coeff[4]
    beta4.b[i] <- fit.boot$coeff[5]
    beta5.b[i] <- fit.boot$coeff[6]
}

p_1 <- (1 + sum(abs(beta1.b) >= abs(beta1))) / (B + 1)
p_2 <- (1 + sum(abs(beta2.b) >= abs(beta2))) / (B + 1)
p_3 <- (1 + sum(abs(beta3.b) >= abs(beta3))) / (B + 1)
p_4 <- (1 + sum(abs(beta4.b) >= abs(beta4))) / (B + 1)
p_5 <- (1 + sum(abs(beta5.b) >= abs(beta5))) / (B + 1)

# number of bootstrap
N <- 1000
PermuteFunction <- function(y = chicken$weight, x = chicken$feed) {
  model.resample = lm(sample(y, replace = F) ~ x)
  fstats = summary(model.resample)$fstat[1]
  return(fstats)
}

fstats = numeric(N)

for (i in 1:N) {
  fstats[i] <- PermuteFunction()
}

p_permute <- (1 + length(fstats[fstats >= summary(fit.lm)$fstat[1]]))/(N + 1)
p_permute

cc <- chicken  |> 
  filter(feed %in% c("soybean", "sunflower"))

cc <- cc |> 
  mutate(feed_dummy = ifelse(feed == "sunflower", 1, 0))

set.seed(123)

# fit model
fit.lm <- lm(weight ~ feed_dummy, data = cc)
beta0 <- summary(fit.lm)$coeff[1,1]
beta1 <- summary(fit.lm)$coeff[2,1]
sigma <- (summary(fit.lm))$sigma

# bootstrap
B <- 1000
beta0.b <- beta1.b <- c(1:B)
n <- length(cc$feed)
weight.b <- c(1:n)
for (i in 1:B) {
  for (j in 1:n) {
    weight.b[j] <- rnorm(1, beta0 + beta1*cc$feed_dummy[j], sigma)
  }
  fit.lm.b <- lm(weight.b ~ cc$feed_dummy)
  beta0.b[i] <- summary(fit.lm.b)$coeff[1,1]
  beta1.b[i] <- summary(fit.lm.b)$coeff[2,1]
}
quantile(beta1.b, probs = c(0.05, 0.95))

# Question 2

computer <- lm(price ~ hd, data = Computers)
summary(computer)

price_predict <- predict(computer, newdata = Computers)

summary(computer)$sigma

# Define the 10-fold cross-validation method
train_control <- trainControl(method = "cv", number = 10)

# Fit the linear model using 10-fold cross-validation
model_cv <- train(price ~ hd, data = Computers, method = "lm", trControl = train_control)

# Calculate the prediction error (RMSE or other metrics)
prediction_error <- model_cv$results$RMSE

n <- length(Computers$price)
beta.cv <- fit.cv <- c(1:n)
x <- Computers$hd
y <- Computers$price
for (i in 1:n) {
    x.cv <- x[ -c(i)]
    y.cv <- y[ -c(i)]
    fit.lm.cv <- lm(y.cv ~ x.cv)
    beta.cv[i] <- fit.lm.cv$coeff[2]
}

plot(beta.cv)

# Question 3

set.seed(123)
n <- length(Computers$price)
index <- c(1:n)
B <- 1000
beta0.b <- beta1.b <- predict.b <- c(1:B)
computers <- Computers |> 
  select(price, hd)
for (i in 1:B) {
index.b <- sample(index, n, replace = TRUE)
computers.b <- computers[index.b, ]
fit.lm.b <- lm(computers.b$price~computers.b$hd)
predict.b[i] <- predict(fit.lm.b, newdata = computers)
beta0.b[i] <- summary(fit.lm.b)$coeff[1,1]
beta1.b[i] <- summary(fit.lm.b)$coeff[2,1]
}
quantile(beta0.b, probs = c(0.025,0.975))
quantile(beta1.b, probs = c(0.025,0.975))

set.seed(123)
n <- length(Computers$price)
index <- c(1:n)
B <- 1000
sebeta0.b <- sebeta1.b <- c(1:B)
computers <- Computers |> 
  select(price, hd)
for (i in 1:B) {
index.b <- sample(index, n, replace = TRUE)
computers.b <- computers[index.b, ]
fit.lm.b <- lm(computers.b$price~computers.b$hd)
sebeta0.b[i] <- summary(fit.lm.b)$coeff[1,2]
sebeta1.b[i] <- summary(fit.lm.b)$coeff[2,2]
}
quantile(sebeta0.b, probs = c(0.025,0.975))
quantile(sebeta1.b, probs = c(0.025,0.975))

set.seed(123)
computers_2k <- Computers |> 
  filter(hd <= 2000)
n <- length(computers_2k$price)
index <- c(1:n)
B <- 1000
sebeta0.b <- sebeta1.b <- c(1:B)
computers <- computers_2k |> 
  select(price, hd)
for (i in 1:B) {
index.b <- sample(index, n, replace = TRUE)
computers.b <- computers[index.b, ]
fit.lm.b <- lm(computers.b$price~computers.b$hd)
sebeta0.b[i] <- summary(fit.lm.b)$coeff[1,2]
sebeta1.b[i] <- summary(fit.lm.b)$coeff[2,2]
}
quantile(sebeta0.b, probs = c(0.025,0.975))
quantile(sebeta1.b, probs = c(0.025,0.975))

# Question 4

set.seed(123)
n <- length(x)
B <- 1000
mean.b <- median.b <- c(1:B)
for (i in 1:B) {
x.b <- sample(x, n, replace = TRUE)
mean.b[i] <- mean(x.b)
median.b[i] <- median(x.b)
}

hist_mean <- ggplot(data.frame(mean.b), aes(x = mean.b)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(x = "Sample mean", y = "Frequency") +
  theme_classic()

hist_med <- ggplot(data.frame(median.b), aes(x = median.b)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(x = "Sample median", y = "Frequency") +
  theme_classic()
hist_mean + hist_med

# Number of bootstrap samples
B <- 1000

# Fit a linear model (mean model)
fit <- lm(x ~ 1)

# Extract residuals
residuals <- fit$residuals

# Initialize vectors to store bootstrap estimates
bootstrap_means <- numeric(B)
bootstrap_medians <- numeric(B)

# Perform semi-parametric bootstrap
set.seed(123)  # For reproducibility
for (i in 1:B) {
  # Resample residuals
  resampled_residuals <- sample(residuals, length(x), replace = TRUE)
  
  # Create bootstrap sample
  bootstrap_sample <- fit$fitted.values + resampled_residuals
  
  # Calculate mean and median of the bootstrap sample
  bootstrap_means[i] <- mean(bootstrap_sample)
  bootstrap_medians[i] <- median(bootstrap_sample)
}

# Estimate standard errors
sd(bootstrap_means)
sd(bootstrap_medians)

# Calculate 95% confidence intervals
quantile(bootstrap_means, c(0.025, 0.975))
quantile(bootstrap_medians, c(0.025, 0.975))

n <- length(x)
mean.jack <- median.jack <- c(1:n)
 for (i in 1:n) {
    x.jack <- x[ -c(i)]
    mean.jack[i] <- mean(x.jack)
    median.jack[i] <- median(x.jack)
 }

# Jackknife estimate of the mean and median
jackknife_mean <- mean(mean.jack)
jackknife_median <- median(median.jack)

# Jackknife bias
bias_jackknife_mean <- (n - 1) * (jackknife_mean - mean(x))
bias_jackknife_median <- (n - 1) * (jackknife_median - median(x))

# Jackknife variance
var_jackknife_mean <- (n - 1) * mean((mean.jack - jackknife_mean)^2)
var_jackknife_median <- (n - 1) * mean((median.jack - jackknife_median)^2)

# Mean Squared Error (MSE)
mse_jackknife_mean <- bias_jackknife_mean^2 + var_jackknife_mean
mse_jackknife_median <- bias_jackknife_median^2 + var_jackknife_median

set.seed(123)
N <- 1000
prob.b <- c(1:N)
for (j in 1:N) {
n <- length(x)
B <- 1000
median.b <- c(1:B)
for (i in 1:B) {
x.b <- sample(x, n, replace = TRUE)
median.b[i] <- median(x.b)
}
prob.b[j] <- mean(median.b < 0)
}
```
